//
B4PDOCU.START

  "UNICODE Character Set" :
  {
    "Documentation":		"General Description", // Required value
    "Feature Names":		[ "UNICODE character set" ],
    "Keywords":			[ "UNICODE", "character set" ],
    "Description 01 Introduction"::

	Beyond4P supports the full UNICODE character set which includes

	* The Basic Multilingual Plane (codes 0 ... 65,535), as well as
	* 16 additional UNICODE planes (codes 65,536 ... 1,114,111).

	In contrast to various other programming languages, Beyond4P considers every UNICODE character as one single
	character.  For example, 'Caf&eacute;' counts 4 characters.  The full character set is available to define
	variable names, table names, table header names, user function names, path and file names, etc.
	Internally, in order to conserve memory needs for large data, all text data is stored and handled in UTF-8 format.

	<br><br>
	Note that Latin, Greek and Cyrillic characters share the same letter, e.g. capital letter 'A'.  Attempting to compare these
	different chracters among them indicate that these characters are different.  As an additional example, 
	the greek <i>mu</i> &mu; and the <i>micro</i> &micro; symbols are different, too.

	+++,

    "Examples 01"::
	inhabitants [ Z√ºrich ] = 402000;
	–ü—ë—Ç—Ä –ß–∞–π–∫o–≤—Å–∫–∏–π [ famous concert ] = Nutcracker; // Piotr Tschaikowski
	echo( inhabitants [ Z√ºrich ] );
	echo( –ü—ë—Ç—Ä –ß–∞–π–∫o–≤—Å–∫–∏–π [ famous concert ] );
	+++,
    "Output 01":		"automatic",



    "Description 02 Line Separator Symbols"::

	Beyond4P is able to load text files containing both "carriage return + line feed" (CR+LF) characters common in Windows systems
	and "line feed only" (LF) common in UNIX/LINUX/MACOS systems.  Internally, Beyond4P considers <i>new line</i> as
	a single character with ANSI/UNICODE 10.  In the language, the reserved keyword 'new line' represents this symbol.
	Unless specified differently, contents saved in Windows systems will use the CR+LF character pair.

	<br><br>
	The legacy Macintosh "carriage return only" (CR) serving as line separator is not supported.

	+++

  },



  "Loading and Saving Files" :
  {
    "Documentation":		"General Description", // Required value
    "Feature Names":		[ "loading files" ],
    "Keywords":			[ "loading files" ],
    "Description 01 Loading Files"::

	Beyond4P checks all files (programs and data) opened for <i>Byte Order Marks</i> (BOM) at the beginning of the file.  The BOM is a UNICODE character and used to differentiate between
	UTF-16 big endian, UTF-16 little endian and UTF-8 file formats.  All formats are supported, whereas UTF-8 is by far the most common UNICODE data storage format
	as it is compatible to various legacy systems supporing 8-byte character sets only.

	<br><br>
	Data transparency: The byte order marks will be recognized and then discarded, i.e. not passed on as special characters to the application.
	<br><br>

	How BOM's are checked in files loaded or opened:
	* If the 2 byte sequence FE FF (hexadecimal) is found, then the text file is in UTF-16 big endian format. 
	* If the 2 byte sequence FF FE (hexadecimal) is found, then the text file is in UTF-16 little endian format.
	* If the 3 byte sequence EF BB BF (hexadecimal) is found, then the text file is in UTF-8 format.
	* Some files contain multiple identical BOMs. They have been sighted in export files from relational databases.

	In case no BOM is found, following checks will be applied throughout the first ca. 4000 ‚Äì 8000 bytes in the file (and not the entire file for performance reasons):
	* NULL-characters / 00 (hexadecimal) in even numbered positions (first byte in file is position 0): File is UTF-16, big endian format.  <br>Example: 00 31   00 30   00 20   20 AC   00 0D   00 0A  (10 ‚Ç¨ followed by new line sequence CR+LF). <br>Even for <i>difficult</i> contents such as pure Chinese text, the UTF-16 will be identified from space symbols (00 20), numeric digits and CR+LF symbols.
	* NULL-characters / 00 (hexadecimal) in odd numbered positions: File is UTF-16, little endian format.  <br>Example: 31 00   30 00   20 00   AC 20   0D 00   0A 00  (10 ‚Ç¨ followed by new line sequence CR+LF)

	If the criteria above do not apply, then the file will be checked for typical UTF-8 patterns in the first 4000-8000 bytes, provided the file contains non-ANSI characters.

	* Presence of non-ANSI symbols which make up typical UTF-8 byte patterns.  These are 2, 3 or 4 symbols with specific binary patterns.
	* Presence of non-ANSI symbols which do not match with UTF-8 byte patterns, e.g. simple 8-bit text in a ISO 8859-1 or WIN 1252 format with single non-ANSI characters
	* If the 1st criteria applies, but the 2nd does not, then the file is in UTF-8 format.
	* If the 2nd criteria applies, but the 1st does not, then the assumption will be non-UNICODE file format WIN 1252 which is the West European 8-bit character set.

	If the input file is in HTML format, then the "charset=‚Ä¶" commands will be checked accordingly.  JSON files are assumed in UTF-8 format by default.
	Ambiguities may still apply in the following case:
	* 1 line of UTF-16 text containing foreign characters only and new line sequence, e.g. one sentence in Greek, Cyrillic or Chinese (without digits, spaces, new lines).
	* 8-bit text file without non-ANSI character in the first 4000-8000 bytes (lots of English text in a huge file, a foreign word such as Caf√© follows in a concluding sentence at the end of the file.

	Ambiguities need to be resolved with the system variable <b>local settings [ input file character set ]</b>.
	As long no clear character format has been identified (e.g. UTF-16 or UTF-8), then the local settings will be referenced.  The initial default value is win1252 (American and West European character set).

	<br><br>
	Supported character sets summarized<br>

	=== 200, 300, 500
	Character Set | Format | Description
	ANSI 		| 8 bits, 7 of them used 	| Traditional ANSI characters.  All non-ANSI characters, including foreign characters, the Euro symbol, etc. are converted into question marks. <br>Examples: E  e
	iso8859-1	| 8 bits			| ANSI characters plus West European character set in the range between 0xA0 (160) and 0xFF (255). <br> This format does not support Windows proprietary character range between 0x80 (128) and 0x9F (159) which affects the Euro symbol (‚Ç¨). <br>Examples: E  e  √â  √©
	win1252		| 8 bits, <br> default setting for Windows | Same as above, but includes Windows proprietary character range so additional punctuation symbols as well as the Euro symbol (‚Ç¨) will be handled correctly. <br>Examples: E  e  √â  √©  ‚Ç¨
	utf-8		| 8 bits			| UNICODE format.  Characters can take 1, 2, 3 or 4 byes.<br>Examples: E  e  √â  √©  ‚Ç¨  ∆è  …ô  ‰∏≠ÂõΩ  êåÑ (also applicable in next rows below)
	utf-16		|16 bits (little endian)	| UNICODE format. Every character contains precisely 2 bytes, starting with the least significant byte.  Surrogate pairs are used for characters outside Basic Multilingual Plane.
	utf-16 big endian | 16 bits (big endian)	| UNICODE format like above, but the two bytes are swapped. Surrogate pairs are used for characters outside Basic Multilingual Plane.
	===

	Note: Microsoft Excel does not understand utf-16 big endian, but understands the remaining UNICODE formats.  Use this format only if the recipient (e.g. a UNIX server) operates on big endian format only.


	+++,

    "Description 02 Saving Files"::

	The chosen file format either depends on the format specified in a parameter in the __table save(__) function call, 
	or it refers to the default setting stored in the __system variable__ <b>local settings [ output file character set ]</b>. If not modified by the user, 
	this file contains the value UTF-8. Files saved in UNICODE-format (UTF-8 and UTF-16 formats) will always begin start with corresponding Byte Order Marks.
	This allows other applications to recognize and interpret the contents correctly.  This does effectively apply to Excel when reading comma or tab separated
	file formats (.csv format).<br>
	<br>Text files under Windows contain "carriage return + line feed" line breaks. 
	<br>Text files under LINUX contain "line feed" line breaks.
	<br>You can change the setting with by setting system variable runtime settings [ crlf ] to true (enabled) or false (disabled)


	+++

  },


  "CSV File Format" :
  {
    "Documentation":		"General Description", // Required value
    "Feature Names":		[ "CSV" ],
    "Keywords":			[ "CSV" ],
    "Description 01 CSV File Format"::

	The CSV (Comma Separated Values) is a simple structured text file format existing since the nostalgic 1970's along with the introduction of FORTRAN 77 and is supported by
	Beyond4P, along with HTML/MHTML, XML and JSON.

	<br><br>
	THe biggest drawback is the ignorance towards country settings.  CSV generated from Excel files use symbols such as
	* List separator (could be commas, semicolons, tab stops, etc.)
	* Decimal point (as used in US, GB, CH) or decimal comma (as used in DE)
	* Thousand separator (various symbols and blanks exist)

	The files can be loaded with and without specifying the list separators.  If no list separator is specified, then Beyond4P tries to automatically detect the
	applicable separators by checking for commas, semicolons and tabs.

	Regarding decimal symbols, the standard function __table load(__) will check for numerals in text and CSV files (but not HMTL files and likes) with decimal 
	commas and convert them to decimal points.  The new function __table load unchanged(__) will not do the conversion.  You may need this if you need to
	discriminate between actual numbers and other numeric data separated with commas but not considered as regular numeric data. 
	<br/>Consider using the __clean numeral(__)
	function to prepare numbers containing commas (and possibly more, such as thousand separators, currency symbols, etc.).
	<br/>Consider using __table save with local decimal separator(__) to save CSV files using the decimal separator symbol applicable with your country settings (comma or point).
	<br/>Consider using __table save with decimal comma(__) to make sure all numbers use decimal commas.
	<br>The original function __table save(__) saves numbers with decimal points, regardless of the applicable country settings.
	
	<br><br>

	Notice on files with fixed column widths: The best is to load these files with "new line" as separator symbol so the table contains one item per row.
	Then create additional columns with dedicated data items using the __left(__) / __middle(__) / __right(__) functions where you can specify numeric column positions, 
	and remove redundant white space symbos with the __trim(__) function.
	
	+++
  },





  "Console Input / Output" :
  {
    "Documentation":		"General Description", // Required value
    "Feature Names":		[ "console" ],
    "Keywords":			[ "console" ],
    "Description 01 Console Output"::

	Even in Windows 7, 8 and 10, the good old console (started with "cmd" or "msdos" or "powershell") contains archaeological software artefacts dating back to 1981
	when the IBM PC has been launched.  Luckily, the recent Windows releases support UTF-8 output to the console.  However, the supported character sets
	are limted to the following:

	* Basic Latin
	* Latin-1 Supplement
	* Latin Extended A
	* Greek (for your fraternity abbreviations)
	* Cyrillic (Russian)
	* A set of additional symbols (also found in win1252 character set) including the Euro currency sign

	Unsupported symbols will automatically be shown as question mark or with a placeholder box.

	+++,

    "Description 02 Console Input"::

	Another relict from 1981: Unfortunately, Microsoft has not done its homework regarding UNICODE input from the keyboard and we need to accept the facts established
	in the year when Ronald Reagan was elected as the President, surely a better choice than since 2016, and foreign characters appeared more foreign at that time than today, too.
	Non-ANSI symbols (accents, umlauts, Euro currency symbol, etc.) are converted with an antiquated MS-DOS character set.  Beyond4P resolves this issue by automatically
	converting entered characters into UNICODE.

	+++
  },



  "Case Sensitivity" :
  {
    "Documentation":		"General Description", // Required value
    "Feature Names":		[ "case sensitivity" ],
    "Keywords":			[ "case sensitivity", "upper case", "lower case" ],
    "Description 01 Case Sensitvity and Conversion"::

	Beyond4P is a case sensitive language.  Case conversions support the important UNICODE character blocks where
	upper/lower case conversion is meaningful.  Accents, Umlauts and other diacrytics will be preserved in case conversions.
	The case conversion is does not follow any specific locales, for example removing accents for capital letters or
	special ruling for the turkish letters &#x0130; / i / I / &#x0131;.  The lower case sharp-s &#x00DF; will not be converted to
	an equivalent upper case character, nor will it be substituted by double-S.

	=== 300, 300, 300
	UNICODE Block | Code Range | Example
	Basic Latin | 00 - 7F (Hex) | A / a
	Latin 1 Supplement | 80 - FF (Hex) | &Auml; / &auml;
	Latin Extended A | 100 - 1FF (Hex) | &#x0102; / &#x0103;
	===
	
	+++
  },




  "HTML Entities" :
  {
    "Documentation":		"General Description", // Required value
    "Feature Names":		[ "entities" ],
    "Keywords":			[ "entities", "html entities" ],
    "Description 01 Entities"::

	Tables in supported HTML / MHTML, as well as softquoted literals (referenced in program code with single quotation marks) support entity references.  Examples:
	=== 100, 100, 600
	Entity		| Character	| Explanation
	&amp;euro;	|	‚Ç¨	| Euro sign (case sensitive, i.e. &Euro; will not be converted)
	&amp;#8364; 	|	‚Ç¨	| ", represented with decimal UNICODE number
	&amp;#x20AC;	|	‚Ç¨	| ", represented with hexadecimal UNICODE number
	&amp;#X20ac;	| 	‚Ç¨	| Same. The letters X and of hexadecimal digits are not case sensitive
	===
	
	+++
  }






B4PDOCU.STOP